{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bffdd4-7a09-4ab3-b9b0-033e96ca704e",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "One of the tasks of the assignment is to evaluate the model. And while Mean Average Error is a robust metric for regression tasks, other metrics can reveal more info about the performance of the model. In this notebook I will look at multiple performance metrics of the model and find the most important. Metrics would be measured on the whole dataset, since difference in loss between test and train during training was not really high, and that approach is not likely to spoil the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f8b1f3-a846-4d84-966b-5363d55e711c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T13:37:27.872469Z",
     "iopub.status.busy": "2023-12-03T13:37:27.871710Z",
     "iopub.status.idle": "2023-12-03T13:37:28.381845Z",
     "shell.execute_reply": "2023-12-03T13:37:28.380359Z",
     "shell.execute_reply.started": "2023-12-03T13:37:27.872469Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "data = pd.read_csv('classical.csv', index_col=0)\n",
    "data.head()\n",
    "\n",
    "def norm_year(year):\n",
    "    return year - 1900\n",
    "    \n",
    "data['23'] = data['23'].apply(norm_year)\n",
    "data['23']\n",
    "X = data.drop(columns=['43'])\n",
    "y = data['43']\n",
    "\n",
    "Xt = torch.tensor(X.values, dtype=torch.float)\n",
    "yt = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "dataset = TensorDataset(Xt, yt)\n",
    "\n",
    "batch_size = 512\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35729ddd-8120-440c-887e-0fdaaaf51442",
   "metadata": {},
   "source": [
    "Let's also take a look on dataset metrics, so we can assess the scores we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e9b9dc1-2618-4e79-b8f1-e184891e5ce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T13:37:29.737662Z",
     "iopub.status.busy": "2023-12-03T13:37:29.736898Z",
     "iopub.status.idle": "2023-12-03T13:37:29.770092Z",
     "shell.execute_reply": "2023-12-03T13:37:29.769351Z",
     "shell.execute_reply.started": "2023-12-03T13:37:29.737662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    99991.000000\n",
       "mean         3.529868\n",
       "std          1.125679\n",
       "min          1.000000\n",
       "25%          3.000000\n",
       "50%          4.000000\n",
       "75%          4.000000\n",
       "max          5.000000\n",
       "Name: 43, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b5fbb54-50a8-4262-b75b-2cef11d347a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T13:37:56.050552Z",
     "iopub.status.busy": "2023-12-03T13:37:56.049053Z",
     "iopub.status.idle": "2023-12-03T13:37:56.059893Z",
     "shell.execute_reply": "2023-12-03T13:37:56.058404Z",
     "shell.execute_reply.started": "2023-12-03T13:37:56.049802Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up the model\n",
    "from torch import nn\n",
    "\n",
    "class RatingModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(RatingModel, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6aa688b7-e7f9-429c-b312-af153affaac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T13:47:50.455990Z",
     "iopub.status.busy": "2023-12-03T13:47:50.455246Z",
     "iopub.status.idle": "2023-12-03T13:47:50.467140Z",
     "shell.execute_reply": "2023-12-03T13:47:50.465648Z",
     "shell.execute_reply.started": "2023-12-03T13:47:50.455990Z"
    }
   },
   "outputs": [],
   "source": [
    "#set up prediction fucntion\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def predict(model, dataloader, device=\"cpu\"):\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            results = None\n",
    "            targets = None\n",
    "            model.eval()  # evaluation mode\n",
    "            loop = tqdm(enumerate(dataloader, 0), total=len(dataloader))\n",
    "            for i, data in loop:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                labels = torch.squeeze(labels)\n",
    "                outputs = torch.squeeze(outputs)\n",
    "\n",
    "                if results is None:\n",
    "                    results = outputs\n",
    "                    targets = labels\n",
    "                else:\n",
    "                    results = torch.cat((results, outputs))\n",
    "                    targets = torch.cat((targets, labels))\n",
    "                \n",
    "        return results.cpu(), targets.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8ae503a-0bd3-4796-ad61-c6ac1c500433",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T13:53:36.007029Z",
     "iopub.status.busy": "2023-12-03T13:53:36.007029Z",
     "iopub.status.idle": "2023-12-03T13:53:37.858453Z",
     "shell.execute_reply": "2023-12-03T13:53:37.856966Z",
     "shell.execute_reply.started": "2023-12-03T13:53:36.007029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0309a276ee2f4dc7a2b773dfa8d934d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the model and get the predictions\n",
    "model = RatingModel(input_dim=43)\n",
    "ckpt = torch.load(\"fc-43-084.pt\") # you may need to change that to best.pt or other\n",
    "model.load_state_dict(ckpt)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "preds, labels = predict(model, dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194d289-54c9-4e00-9af1-34e8dcadc38a",
   "metadata": {},
   "source": [
    "First, let's re-evaluate the model using Mean Average Error metric so we have it on hand to compare to other metrics. Pytorch implements this loss function as L1Loss, so we can easily use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfce8989-108e-4ee4-94ec-32866c0bb4a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T13:53:41.941835Z",
     "iopub.status.busy": "2023-12-03T13:53:41.940502Z",
     "iopub.status.idle": "2023-12-03T13:53:41.949973Z",
     "shell.execute_reply": "2023-12-03T13:53:41.948491Z",
     "shell.execute_reply.started": "2023-12-03T13:53:41.941835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8477112650871277\n"
     ]
    }
   ],
   "source": [
    "MAEScore = nn.functional.l1_loss(preds, labels).item()\n",
    "print(MAEScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb266be4-9ee0-4add-8b37-acab618a2bd6",
   "metadata": {},
   "source": [
    "The result we get is almost identical to the result we got during training, so no issues here.\n",
    "\n",
    "Next evaluation function we'll try is Mean Squared Error. I tried using it as a loss function in the training stage, but it ended up not converging. But we still can use it as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ba9b160-691f-45c6-b4a3-a1f0c4df68bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T13:55:08.141669Z",
     "iopub.status.busy": "2023-12-03T13:55:08.140933Z",
     "iopub.status.idle": "2023-12-03T13:55:08.150075Z",
     "shell.execute_reply": "2023-12-03T13:55:08.148589Z",
     "shell.execute_reply.started": "2023-12-03T13:55:08.141669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1995724439620972\n"
     ]
    }
   ],
   "source": [
    "MSEScore = nn.functional.mse_loss(preds, labels).item()\n",
    "print(MSEScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00707790-4015-453b-a76e-f42f4460dc77",
   "metadata": {},
   "source": [
    "The result we get there is higher than the MAE. That means that the model often has error of more than 1, which might suggest presence of outliers of dataset (which MSE is susceptible to).\n",
    "\n",
    "Now lets try calculating Accuracy of the model. The task of predicting the rating is not classification task, but rather a regression one, but since movie ratings in the dataset are discrete, we can use them as 'classes' to calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "808c76c3-caf3-4642-a602-72c8e8bd193a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:15:15.716069Z",
     "iopub.status.busy": "2023-12-03T14:15:15.715320Z",
     "iopub.status.idle": "2023-12-03T14:15:15.724090Z",
     "shell.execute_reply": "2023-12-03T14:15:15.722814Z",
     "shell.execute_reply.started": "2023-12-03T14:15:15.716069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3527817503575322\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_preds = preds.numpy()\n",
    "np_labels = labels.numpy()\n",
    "\n",
    "Accuracy = np.sum(np_preds.round() == np_labels) / len(np_preds)\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee02cbf-a99e-461e-8ce2-621e6959fc11",
   "metadata": {},
   "source": [
    "Accuracy of 35% is not really high, so to better assess perforamce of the model, let's comapre it to performance of random guessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "327a3dc4-74f5-47d6-a029-57a38d1f7f33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:27:40.785195Z",
     "iopub.status.busy": "2023-12-03T14:27:40.784445Z",
     "iopub.status.idle": "2023-12-03T14:27:40.796273Z",
     "shell.execute_reply": "2023-12-03T14:27:40.794781Z",
     "shell.execute_reply.started": "2023-12-03T14:27:40.785195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.388507604598999\n",
      "MSE: 2.8849971294403076\n",
      "Accuracy: 0.21556940124611215\n"
     ]
    }
   ],
   "source": [
    "rnd_preds = torch.rand(preds.shape) * 4 + 1\n",
    "print(f'MAE: {nn.functional.l1_loss(rnd_preds, labels).item()}')\n",
    "print(f'MSE: {nn.functional.mse_loss(rnd_preds, labels).item()}')\n",
    "np_rnd = rnd_preds.numpy()\n",
    "print(f'Accuracy: {np.sum(np_rnd.round() == np_labels) / len(np_rnd)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3374d6-2f33-4081-a050-32573af6c2e4",
   "metadata": {},
   "source": [
    "Performance of the model is noticeably better than random guessing, so we can consider it a success.\n",
    "\n",
    "Out of all metrics tested, Accuracy was the least useful metric, since we consider the task of movie recomendation to be regression task, not a classification one.\n",
    "MSE and MAE metrics are both valuable, since MAE shows average error, and allows us to build an expectation of how off the model would be on average.\n",
    "MSE is very similar to MAE, but since the error is squared, it can be compared to MAE to see how often model has really high error. \n",
    "\n",
    "I will use MAE and MSE as final evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
